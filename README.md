# ğŸ¢ Company Policy Chatbot (Local RAG System)

A **local, privacy-preserving company policy chatbot** built using **FastAPI, ChromaDB, and Ollama**.  
Organizations can upload internal policy documents (PDFs), and employees can ask questions that are answered **strictly based on the uploaded policies**, with **no hallucinations**.

This project implements a **Retrieval-Augmented Generation (RAG)** pipeline that runs **entirely on-prem / locally**, making it suitable for sensitive enterprise data (HR, Legal, Compliance).

---

## âœ¨ Key Features

- ğŸ“„ Upload company policy PDFs (HR, compliance, employee handbooks)
- ğŸ” Semantic search over policies using vector embeddings
- ğŸ§  Local LLM inference via Ollama (no cloud APIs)
- ğŸ§· Grounded answers with source references
- ğŸš« Safe refusals when a policy is not explicitly covered
- ğŸ¢ Multi-company support via metadata filtering
- âš™ï¸ Modular, extensible backend architecture

---

## ğŸ§  High-Level Architecture

```
PDF Upload
   â†“
Text Extraction
   â†“
Chunking
   â†“
Embeddings
   â†“
ChromaDB (Vector Store)
   â†“
User Question
   â†“
Semantic Retrieval + Reranking
   â†“
Prompt Construction
   â†“
Local LLM (Ollama / LLaMA 3)
   â†“
Answer + Citations
```

---

## ğŸ§© Tech Stack

| Component | Technology |
|---------|------------|
| API | FastAPI |
| Vector Database | ChromaDB |
| Embeddings | SentenceTransformers (BGE) |
| LLM Runtime | Ollama |
| LLM Model | LLaMA 3 (8B) |
| PDF Parsing | pypdf |
| Language | Python |
| OS | Windows (tested), portable |

---

## ğŸ“ Project Structure

```
company-llm-bot/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api.py          # FastAPI endpoints
â”‚   â”œâ”€â”€ ingest.py       # PDF ingestion pipeline
â”‚   â”œâ”€â”€ rag.py          # Retrieval + RAG logic
â”‚   â”œâ”€â”€ llm_client.py   # Ollama client
â”‚   â”œâ”€â”€ config.py       # Config & paths
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/            # Uploaded policy PDFs (source of truth)
â”‚   â””â”€â”€ chroma/         # ChromaDB persistent storage
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ”„ End-to-End Workflow

### 1ï¸âƒ£ Policy Ingestion (`POST /ingest/pdf`)
1. Admin uploads a policy PDF  
2. PDF is saved to `data/raw/` (audit & reproducibility)  
3. Text is extracted page-by-page  
4. Text is chunked with overlap  
5. Each chunk is embedded into vectors  
6. Chunks + metadata are stored in ChromaDB  

### 2ï¸âƒ£ Question Answering (`POST /chat`)
1. User submits a question  
2. Question is embedded into a vector  
3. ChromaDB retrieves top-N relevant chunks  
4. Retrieved chunks are **reranked** for precision  
5. A grounded prompt is built  
6. Ollama runs the local LLM  
7. Answer is returned with citations or a safe refusal  

---

## ğŸ›¡ï¸ Safety & Reliability

- âŒ No hallucinated answers  
- âœ… Answers strictly use retrieved policy text  
- âš ï¸ Explicit â€œnot foundâ€ responses when policies donâ€™t exist  
- ğŸ” Source chunk references included  
- ğŸ” All data stays local  

This behavior is **intentional** and **enterprise-safe**.

---

## ğŸ§  Why RAG Over Fine-Tuning?

Fine-tuning is not well-suited for internal policy systems.

**Retrieval-Augmented Generation (RAG)** was chosen because:

- ğŸ“„ **Policies change frequently** â†’ RAG updates instantly by re-indexing documents  
- ğŸš« **Hallucination prevention** â†’ LLM only sees retrieved policy text  
- ğŸ” **Auditability** â†’ answers are traceable to specific document chunks  
- ğŸ”„ **Model flexibility** â†’ swap LLMs without retraining  
- ğŸ” **On-prem deployment** â†’ no sensitive data sent to cloud APIs  

> **Fine-tuning teaches a model how to speak.  
> RAG teaches a system what to know â€” safely.**

---

## ğŸš€ Setup Instructions

### 1ï¸âƒ£ Create virtual environment
```powershell
python -m venv venv
.\venv\Scripts\Activate.ps1
```

### 2ï¸âƒ£ Install dependencies
```powershell
pip install -r requirements.txt
```

### 3ï¸âƒ£ Install Ollama & pull model
Download Ollama from: https://ollama.com/download

```powershell
ollama pull llama3:8b
```

### 4ï¸âƒ£ Run the API
```powershell
python -m uvicorn app.api:app --reload
```

Open:
ğŸ‘‰ http://127.0.0.1:8000/docs

---

## ğŸ§ª Example Usage

### Upload a policy PDF
`POST /ingest/pdf`

- `company_id`: `acme`
- `doc_name`: `handbook_2025`
- `file`: employee handbook PDF

### Ask a question
`POST /chat`

```json
{
  "company_id": "acme",
  "question": "What is the sick leave policy?"
}
```

---

## ğŸ“Œ Current Limitations

- OCR not enabled for scanned PDFs  
- Authentication & role-based access not yet implemented  
- Page/section-level citations pending  
- No frontend UI (API-only)  

---

## ğŸ”œ Planned Improvements

- ğŸ” JWT authentication & role-based access  
- ğŸ“‘ Page + section citations  
- ğŸ§  Hybrid retrieval (semantic + keyword)  
- ğŸ§ª Evaluation harness for answer quality  
- ğŸ–¥ï¸ Web UI (Streamlit / React)  
- ğŸ“¦ Dockerized deployment  
- ğŸ“š Document versioning & lifecycle management  

---
