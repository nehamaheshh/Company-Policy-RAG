# ğŸ¢ Company Policy Chatbot (Local RAG System)

A **local, privacy-preserving company policy chatbot** built with **FastAPI, ChromaDB, and Ollama**.  
Organizations can upload internal policy documents (PDFs), and employees can ask questions that are answered **strictly based on the uploaded policies**, with **grounded citations** and **safe refusals** when content is not present.

This project implements a **Retrieval-Augmented Generation (RAG)** pipeline that runs **entirely on-prem / locally**, making it suitable for sensitive enterprise data (HR, Legal, Compliance).

---

## âœ¨ Key Features

- ğŸ“„ Upload policy PDFs via API (supports multi-company ingestion)
- ğŸ” Semantic retrieval using embeddings + ChromaDB
- ğŸ¯ **Reranking** for higher answer precision (retrieve wide â†’ rerank smart â†’ answer narrow)
- ğŸ§  Local LLM inference via Ollama (no cloud APIs)
- ğŸ§· Grounded answers with source metadata
- ğŸš« Safe refusals when a policy is not explicitly covered
- ğŸ–¥ï¸ Streamlit frontend for upload + chat
- âš™ï¸ Modular backend (easy to extend: auth, OCR, versioning)

---

## ğŸ§  High-Level Architecture

PDF Upload
â†“
Save raw PDF (audit & reproducibility)
â†“
Text Extraction
â†“
Chunking
â†“
Embeddings
â†“
ChromaDB (Vector Store)
â†“
User Question
â†“
Question Embedding
â†“
Retrieve N candidates
â†“
Rerank â†’ Top K evidence
â†“
Prompt Construction (grounded)
â†“
Local LLM (Ollama / LLaMA 3)
â†“
Answer + Sources


---

## ğŸ§© Tech Stack

| Component | Technology |
|---------|------------|
| Backend API | FastAPI |
| Vector Database | ChromaDB (persistent) |
| Embeddings | SentenceTransformers (BGE) |
| Reranker | Cross-Encoder (MS MARCO MiniLM) |
| LLM Runtime | Ollama |
| LLM Model | LLaMA 3 (8B) |
| PDF Parsing | pypdf |
| Frontend | Streamlit |
| OS | Windows (tested), portable |

---

## ğŸ“ Project Structure

company-llm-bot/
â”œâ”€â”€ app/
â”‚ â”œâ”€â”€ api.py # FastAPI endpoints (/ingest/pdf, /chat)
â”‚ â”œâ”€â”€ ingest.py # PDF ingestion pipeline
â”‚ â”œâ”€â”€ rag.py # Retrieval + reranking + prompt construction
â”‚ â”œâ”€â”€ llm_client.py # Ollama client
â”‚ â”œâ”€â”€ config.py # Config & paths
â”‚ â””â”€â”€ init.py
â”œâ”€â”€ frontend/
â”‚ â””â”€â”€ app.py # Streamlit UI (upload + chat)
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/ # Uploaded policy PDFs (ignored in git)
â”‚ â””â”€â”€ chroma/ # ChromaDB storage (ignored in git)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


---

## ğŸ”„ End-to-End Workflow

### 1ï¸âƒ£ Policy Ingestion (`POST /ingest/pdf`)
1. Admin uploads a policy PDF  
2. PDF is saved to `data/raw/` for auditability  
3. Text is extracted page-by-page  
4. Text is chunked with overlap  
5. Each chunk is embedded into vectors  
6. Chunks + metadata are stored in ChromaDB  

### 2ï¸âƒ£ Question Answering (`POST /chat`)
1. User submits a question  
2. Question is embedded into a vector  
3. ChromaDB retrieves **N candidate chunks**  
4. Candidates are **reranked** and top **K chunks** are selected  
5. A grounded prompt is constructed  
6. Ollama generates a response locally  
7. Answer is returned with sources and confidence  

---

## ğŸ§  Why RAG Over Fine-Tuning?

Fine-tuning is not ideal for internal policy systems where content changes frequently and auditability matters.

**RAG was chosen because:**

- ğŸ“„ **Instant updates** via document re-indexing
- ğŸš« **Lower hallucination risk**
- ğŸ” **Traceable answers** with document sources
- ğŸ”„ **Model flexibility** without retraining
- ğŸ” **On-prem privacy** for sensitive data

> **Fine-tuning teaches a model how to speak.  
> RAG teaches a system what to know â€” safely.**

---

## ğŸš€ Setup (Windows)

### 1ï¸âƒ£ Create & activate virtual environment
```powershell
python -m venv venv
.\venv\Scripts\Activate.ps1



### 2ï¸âƒ£ Install dependencies
pip install -r requirements.txt


### 3ï¸âƒ£ Install Ollama & pull model
ollama pull llama3:8b


### â–¶ï¸ Run Backend (FastAPI)
python -m uvicorn app.api:app --reload
Swagger UI:
http://127.0.0.1:8000/docs

### ğŸ–¥ï¸ Run Frontend (Streamlit)
python -m streamlit run frontend/app.py
UI:
http://localhost:8501
```

ğŸ“Œ Current Limitations:

OCR not enabled for scanned PDFs
Authentication & role-based access not implemented
Page/section-level citations pending
No automated evaluation suite yet



